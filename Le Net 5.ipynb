{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5\n",
    "LeNet is a convolutional network architecture developed in 1998, that was primarily used for handwritten digit detection. LeNet was widely used to read the handwriting on checks for the U.S. Treasury. \n",
    "\n",
    "### Architecture\n",
    "LeNet consists of 8 layers, 1 being the input, 3 of which are convolutional, 2 are average pooling, and two fully connected layers. By today's standards, this is a relatively simple and straightforward architecture. Our implementation of LeNet's architecture using Tensorflow is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#First layer is the input, was originally 28x28 by padded on each side by 2 pixels.\n",
    "#This is the only layer to use any padding, so the overall size of the layers shrink at every stage\n",
    "X_inp = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "y_inp = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "#First convolutional layer. The arguments for tensorflows conv2d function from layers that we use are:\n",
    "#input: the input layer\n",
    "#filters: the number of output maps generated\n",
    "#kernel_size: the width and height of the kernel. In this network, the width and height are always the same,\n",
    "#so we just specify one number\n",
    "#strides: the horizontal and vertical stride, a tuple generally, but a single number represents same width + height\n",
    "#padding: \"valid\" = no padding, may ignore some rows and columns at bottom of image\n",
    "#         \"safe\" = padding, adds rows and columns if necessary based on the stride\n",
    "#activation: the activation function used by the layer\n",
    "\n",
    "#Conv1 input size: 32 x 32, output size: 28 x 28 x 6\n",
    "conv1 = tf.layers.conv2d(X_inp, filters=6, kernel_size=5,  strides=1, padding=\"valid\", activation=tf.nn.relu) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pooling\n",
    "LeNet uses average pooling as as opposed to max pooling. The idea is the same, only that the average value within the pool is taken for the layer instead of the maximum. Similar to a kernel with 1 / (pool_width) * 1 / (pool_height) for its entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_pooling2d args: \n",
    "#input: input layer\n",
    "#pool_size: width, height of pool size, single number means same width + height\n",
    "#strides: horiz and vertical stride, 1 number = same width + height\n",
    "\n",
    "#avg_pool input size: 28 x 28 x 6, output size: 14 x 14 x 6\n",
    "#pool size of (2,2) and stride size of (2,2) halves the dimensions of the previous layer\n",
    "avg_pool = tf.layers.average_pooling2d(conv1, pool_size=2, strides=2)\n",
    "\n",
    "#Conv2 input size: 14 x 14 x 6, output size: 10 x 10 x 16\n",
    "#because of padding valid and kernel size (5,5), only the first 10 / 14 pixels of each row and column are used\n",
    "#in this layer\n",
    "conv2 = tf.layers.conv2d(avg_pool, filters=16, kernel_size = 5, strides=1,\n",
    "                       padding=\"valid\", activation = tf.nn.relu)\n",
    "#avg_pool2 input size: 10 x 10 x 16, output size: 5 x 5 x 16\n",
    "avg_pool2 = tf.layers.average_pooling2d(conv2, pool_size = 2, strides=2)\n",
    "\n",
    "#Conv3 input size: 5 x 5 x 16, output size: 1 x 1 x 120\n",
    "#A single kernel of size 5x5 results in 16 1x1 outputs, with 120 filters applied to each of them\n",
    "conv3 = tf.layers.conv2d(avg_pool2, filters=120, kernel_size = 5, strides=1,\n",
    "                      padding=\"valid\", activation = tf.nn.relu)\n",
    "\n",
    "#Reshape the convolutional layer to have size of 400x120 for use in fully connected layer\n",
    "flat = tf.reshape(conv3, [-1, 120])\n",
    "\n",
    "#fully connected dense layer. size: 120x84\n",
    "dense = tf.layers.dense(inputs=flat, units=84, activation=tf.nn.relu)\n",
    "\n",
    "#output layer: inputs to softmax. size: 84 x 10\n",
    "logits = tf.layers.dense(dense, units=10)\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "predict = tf.argmax(softmax, axis=1)\n",
    "\n",
    "y_labels = tf.one_hot(y_inp, 10)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_labels)\n",
    "#loss function is the mean cross entropy for softmax applied to the output layer\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#use adam optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "To make the training data more complete and have the CNN be robust to minor variance in the input, we augment the training data with random rotations, shears, and zooms, using the keras preprocessing library. We do this as an optional first stage in our training pipeline, where the user can enter parameters that affect which transformations get applied and how they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(self, X, y):\n",
    "    #print(\"before TF, shapes = %s, %s\" % (X.shape, y.shape))\n",
    "        X_aug = []\n",
    "        y_aug = []\n",
    "\n",
    "        for x, label in zip(X, y):\n",
    "            # include originals\n",
    "            X_aug.append(x)\n",
    "            y_aug.append(label)\n",
    "\n",
    "            # random rotations up to -60/60 degrees\n",
    "            for degree in range(*self.rotation):\n",
    "                rotated = tf.contrib.keras.preprocessing.image.random_rotation(\n",
    "                            x, degree, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                X_aug.append(rotated)\n",
    "                y_aug.append(label)\n",
    "\n",
    "\n",
    "                if self.rot_and_shear:\n",
    "                    # random shears up to 40% intensity        \n",
    "                    for sh in range(*self.shear):\n",
    "                        sh /= 10\n",
    "                        sheared = tf.contrib.keras.preprocessing.image.random_shear(\n",
    "                                rotated, sh, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                        X_aug.append(sheared)\n",
    "                        y_aug.append(label)\n",
    "\n",
    "            # random shifts 20% left, right, up or down\n",
    "            for xsh in range(*self.x_shifts):\n",
    "                xsh /= 10\n",
    "                for ysh in range(*self.y_shifts):\n",
    "                    ysh /= 10\n",
    "                    shifted = tf.contrib.keras.preprocessing.image.random_shift(\n",
    "                            x, xsh, ysh, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                    X_aug.append(shifted)\n",
    "                    y_aug.append(label)\n",
    "\n",
    "            # random zoom up to 20%\n",
    "            # zoomed = tf.contrib.keras.preprocessing.image.random_zoom(\n",
    "            #         x, (0.9, 1.0), row_axis=0, col_axis=1, channel_axis=2)\n",
    "            # X_aug.append(zoomed)\n",
    "            # y_aug.append(label)\n",
    "\n",
    "        X_aug = np.array(X_aug)\n",
    "        y_aug = np.array(y_aug)\n",
    "\n",
    "        # print(\"after TF, shapes = %s, %s\" % (X_aug.shape, y_aug.shape))\n",
    "        return (X_aug, y_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Class as SkLearn Estimator + Pipeline\n",
    "We made a class that wraps LeNet as a sklearn estimator, and performed a gridsearch in conjunction with the data augmentation transformer to find an optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from ext_test import kaggle_test\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "##LOAD INPUT\n",
    "X = np.load(\"x_mnist1000.npy\")\n",
    "X = X.reshape((-1, 28, 28, 1))\n",
    "y = np.load(\"y_mnist1000.npy\")\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "indices = np.random.permutation(len(X))\n",
    "\n",
    "valid_ind = indices[:100]\n",
    "test_ind = indices[100:200]\n",
    "train_ind = indices[200:]\n",
    "\n",
    "X_train = X[train_ind]\n",
    "y_train = y[train_ind]\n",
    "\n",
    "X_validation = X[valid_ind]\n",
    "y_validation = y[valid_ind]\n",
    "\n",
    "X_test = X[test_ind]\n",
    "y_test = y[test_ind]\n",
    "\n",
    "\n",
    "\n",
    "default_activations = [\n",
    "    \"conv1\",\n",
    "    \"avg_pool\",\n",
    "    \"conv2\",\n",
    "    \"avg_pool2\",\n",
    "    \"conv3\",\n",
    "    \"dense\",\n",
    "]\n",
    "\n",
    "elu = {x: tf.nn.elu for x in default_activations}\n",
    "selu = {x: tf.nn.selu for x in default_activations}\n",
    "relu = {x: tf.nn.relu for x in default_activations}\n",
    "tanh= {x: tf.nn.tanh for x in default_activations}\n",
    "\n",
    "param_grid = {\n",
    "    \"aug__rotation\": [[-60, 60, 10], [-30, 30, 10]],\n",
    "    \"aug__shear\": [[0, 50, 10], [0, 20, 10]],\n",
    "    \"aug__rot_and_shear\": [True, False],\n",
    "    \"clf__activations\": [selu, elu, tanh],\n",
    "    \"clf__dropout_rate\": [0.25, 0.35, 0.45],\n",
    "    \"clf__batch_size\": [500],\n",
    "    \"clf__using_da\": [True],\n",
    "}\n",
    "\n",
    "da = DataAugment()\n",
    "clf = CNNClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"aug\", da),\n",
    "        (\"clf\", clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "now = datetime.now().strftime(\"%H-%M-%S\")\n",
    "log_name = \"./logs/cnn_log_\" + now + \".log\"\n",
    "with open(log_name, \"w\") as log:\n",
    "    log.write(\"Param grid:\\n\")\n",
    "    for p in param_grid:\n",
    "        log.write(\"%s: %s\\n\" % (p, param_grid[p]))\n",
    " \n",
    "    gs = LoggingGridSearch(pipe, param_grid=param_grid)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_msg = \"best score: %s, params: %s\" % (gs.best_score_, gs.best_estimator_.get_params())\n",
    "    print(best_msg)\n",
    "    log.write(best_msg)\n",
    "\n",
    "    test_msg = \"Test accuracy: %s\" % gs.best_estimator_.score(X_test, y_test)\n",
    "    print(test_msg)\n",
    "    log.write(test_msg)\n",
    "    \n",
    "df = pd.DataFrame(gs.cv_results_)\n",
    "df.to_csv(\"./gs_results/\" + now + \".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters + Performance Plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
