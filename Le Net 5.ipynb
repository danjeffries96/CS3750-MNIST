{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5\n",
    "LeNet is a convolutional network architecture developed in 1998, that was primarily used for handwritten digit detection. LeNet was widely used to read the handwriting on checks for the U.S. Treasury. \n",
    "\n",
    "### Architecture\n",
    "LeNet consists of 8 layers, 1 being the input, 3 of which are convolutional, 2 are average pooling, and two fully connected layers. By today's standards, this is a relatively simple and straightforward architecture. Our implementation of LeNet's architecture using Tensorflow is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#First layer is the input, was originally 28x28 by padded on each side by 2 pixels.\n",
    "#This is the only layer to use any padding, so the overall size of the layers shrink at every stage\n",
    "X_inp = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "y_inp = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "#First convolutional layer. The arguments for tensorflows conv2d function from layers that we use are:\n",
    "#input: the input layer\n",
    "#filters: the number of output maps generated\n",
    "#kernel_size: the width and height of the kernel. In this network, the width and height are always the same,\n",
    "#so we just specify one number\n",
    "#strides: the horizontal and vertical stride, a tuple generally, but a single number represents same width + height\n",
    "#padding: \"valid\" = no padding, may ignore some rows and columns at bottom of image\n",
    "#         \"safe\" = padding, adds rows and columns if necessary based on the stride\n",
    "#activation: the activation function used by the layer\n",
    "\n",
    "#Conv1 input size: 32 x 32, output size: 28 x 28 x 6\n",
    "conv1 = tf.layers.conv2d(X_inp, filters=6, kernel_size=5,  strides=1, padding=\"valid\", activation=tf.nn.relu) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pooling\n",
    "LeNet uses average pooling as as opposed to max pooling. The idea is the same, only that the average value within the pool is taken for the layer instead of the maximum. Similar to a kernel with 1 / (pool_width) * 1 / (pool_height) for its entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_pooling2d args: \n",
    "#input: input layer\n",
    "#pool_size: width, height of pool size, single number means same width + height\n",
    "#strides: horiz and vertical stride, 1 number = same width + height\n",
    "\n",
    "#avg_pool input size: 28 x 28 x 6, output size: 14 x 14 x 6\n",
    "#pool size of (2,2) and stride size of (2,2) halves the dimensions of the previous layer\n",
    "avg_pool = tf.layers.average_pooling2d(conv1, pool_size=2, strides=2)\n",
    "\n",
    "#Conv2 input size: 14 x 14 x 6, output size: 10 x 10 x 16\n",
    "#because of padding valid and kernel size (5,5), only the first 10 / 14 pixels of each row and column are used\n",
    "#in this layer\n",
    "conv2 = tf.layers.conv2d(avg_pool, filters=16, kernel_size = 5, strides=1,\n",
    "                       padding=\"valid\", activation = tf.nn.relu)\n",
    "#avg_pool2 input size: 10 x 10 x 16, output size: 5 x 5 x 16\n",
    "avg_pool2 = tf.layers.average_pooling2d(conv2, pool_size = 2, strides=2)\n",
    "\n",
    "#Conv3 input size: 5 x 5 x 16, output size: 1 x 1 x 120\n",
    "#A single kernel of size 5x5 results in 16 1x1 outputs, with 120 filters applied to each of them\n",
    "conv3 = tf.layers.conv2d(avg_pool2, filters=120, kernel_size = 5, strides=1,\n",
    "                      padding=\"valid\", activation = tf.nn.relu)\n",
    "\n",
    "#Reshape the convolutional layer to have size of 1920x120 for use in fully connected layer\n",
    "flat = tf.reshape(conv3, [-1, 120])\n",
    "\n",
    "#fully connected dense layer. size: 120x84\n",
    "dense = tf.layers.dense(inputs=flat, units=84, activation=tf.nn.relu)\n",
    "\n",
    "#output layer: inputs to softmax. size: 84 x 10\n",
    "logits = tf.layers.dense(dense, units=10)\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "predict = tf.argmax(softmax, axis=1)\n",
    "\n",
    "y_labels = tf.one_hot(y_inp, 10)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_labels)\n",
    "#loss function is the mean cross entropy for softmax applied to the output layer\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#use adam optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet class\n",
    "We made a class that wraps lenet as a sklearn estimator, and example of using it in a grid search is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cnn_wrapper import CNNClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = np.load(\"x_mnist1000.npy\")\n",
    "X = X.reshape((-1, 28, 28, 1))\n",
    "y = np.load(\"y_mnist1000.npy\")\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "indices = np.random.permutation(len(X))\n",
    "train_indices = indices[:800]\n",
    "valid_ind = indices[800:900]\n",
    "test_ind = indices[900:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_validation = X[valid_ind]\n",
    "y_validation = y[valid_ind]\n",
    "\n",
    "X_test = X[test_ind]\n",
    "y_test = y[test_ind]\n",
    "\n",
    "# pad for lenet\n",
    "pad_dims = ((0, 0), (2, 2), (2, 2), (0, 0))\n",
    "X_train = np.pad(X_train, pad_dims, \"constant\")\n",
    "X_validation = np.pad(X_validation, pad_dims, \"constant\")\n",
    "X_test = np.pad(X_test, pad_dims, \"constant\")\n",
    "\n",
    "N = len(X_train)\n",
    "BATCH_SIZE = N // 10 \n",
    "\n",
    "clf = CNNClassifier(batch_size=BATCH_SIZE)\n",
    "\n",
    "param_grid = {\n",
    "    \"verbose\":  [2],\n",
    "    \"activation\": [tf.nn.tanh, tf.nn.relu, tf.nn.elu],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, param_grid=param_grid, n_jobs=-1)\n",
    "gs.fit(X_train, y_train, n_epochs=10, X_valid=X_validation, y_valid=y_validation)\n",
    "print(\"best score, params:\", gs.best_score_, \",\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
