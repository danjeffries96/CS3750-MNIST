{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5\n",
    "LeNet is a convolutional network architecture developed in 1998, that was primarily used for handwritten digit detection. LeNet was widely used to read the handwriting on checks for the U.S. Treasury. \n",
    "\n",
    "### Architecture\n",
    "LeNet consists of 8 layers, 1 being the input, 3 of which are convolutional, 2 are average pooling, and two fully connected layers. By today's standards, this is a relatively simple and straightforward architecture. Our implementation of LeNet's architecture using Tensorflow is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#First layer is the input, was originally 28x28 by padded on each side by 2 pixels.\n",
    "#This is the only layer to use any padding, so the overall size of the layers shrink at every stage\n",
    "X_inp = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "y_inp = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "#First convolutional layer. The arguments for tensorflows conv2d function from layers that we use are:\n",
    "#input: the input layer\n",
    "#filters: the number of output maps generated\n",
    "#kernel_size: the width and height of the kernel. In this network, the width and height are always the same,\n",
    "#so we just specify one number\n",
    "#strides: the horizontal and vertical stride, a tuple generally, but a single number represents same width + height\n",
    "#padding: \"valid\" = no padding, may ignore some rows and columns at bottom of image\n",
    "#         \"safe\" = padding, adds rows and columns if necessary based on the stride\n",
    "#activation: the activation function used by the layer\n",
    "\n",
    "#Conv1 input size: 32 x 32, output size: 28 x 28 x 6\n",
    "conv1 = tf.layers.conv2d(X_inp, filters=6, kernel_size=5,  strides=1, padding=\"valid\", activation=tf.nn.relu) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pooling\n",
    "LeNet uses average pooling as as opposed to max pooling. The idea is the same, only that the average value within the pool is taken for the layer instead of the maximum. Similar to a kernel with 1 / (pool_width) * 1 / (pool_height) for its entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_pooling2d args: \n",
    "#input: input layer\n",
    "#pool_size: width, height of pool size, single number means same width + height\n",
    "#strides: horiz and vertical stride, 1 number = same width + height\n",
    "\n",
    "#avg_pool input size: 28 x 28 x 6, output size: 14 x 14 x 6\n",
    "#pool size of (2,2) and stride size of (2,2) halves the dimensions of the previous layer\n",
    "avg_pool = tf.layers.average_pooling2d(conv1, pool_size=2, strides=2)\n",
    "\n",
    "#Conv2 input size: 14 x 14 x 6, output size: 10 x 10 x 16\n",
    "#because of padding valid and kernel size (5,5), only the first 10 / 14 pixels of each row and column are used\n",
    "#in this layer\n",
    "conv2 = tf.layers.conv2d(avg_pool, filters=16, kernel_size = 5, strides=1,\n",
    "                       padding=\"valid\", activation = tf.nn.relu)\n",
    "#avg_pool2 input size: 10 x 10 x 16, output size: 5 x 5 x 16\n",
    "avg_pool2 = tf.layers.average_pooling2d(conv2, pool_size = 2, strides=2)\n",
    "\n",
    "#Conv3 input size: 5 x 5 x 16, output size: 1 x 1 x 120\n",
    "#A single kernel of size 5x5 results in 16 1x1 outputs, with 120 filters applied to each of them\n",
    "conv3 = tf.layers.conv2d(avg_pool2, filters=120, kernel_size = 5, strides=1,\n",
    "                      padding=\"valid\", activation = tf.nn.relu)\n",
    "\n",
    "#Reshape the convolutional layer to have size of 400x120 for use in fully connected layer\n",
    "flat = tf.reshape(conv3, [-1, 120])\n",
    "\n",
    "#fully connected dense layer. size: 120x84\n",
    "dense = tf.layers.dense(inputs=flat, units=84, activation=tf.nn.relu)\n",
    "\n",
    "#output layer: inputs to softmax. size: 84 x 10\n",
    "logits = tf.layers.dense(dense, units=10)\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "predict = tf.argmax(softmax, axis=1)\n",
    "\n",
    "y_labels = tf.one_hot(y_inp, 10)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_labels)\n",
    "#loss function is the mean cross entropy for softmax applied to the output layer\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#use adam optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "To make the training data more complete and have the CNN be robust to minor variance in the input, we augment the training data with random rotations, shears, and translations, using the keras preprocessing library. We do this as an optional first stage in our training pipeline, where the user can enter parameters that affect which transformations get applied and how they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(self, X, y):\n",
    "    #print(\"before TF, shapes = %s, %s\" % (X.shape, y.shape))\n",
    "        X_aug = []\n",
    "        y_aug = []\n",
    "\n",
    "        for x, label in zip(X, y):\n",
    "            # include originals\n",
    "            X_aug.append(x)\n",
    "            y_aug.append(label)\n",
    "\n",
    "            # random rotations up to -60/60 degrees\n",
    "            for degree in range(*self.rotation):\n",
    "                rotated = tf.contrib.keras.preprocessing.image.random_rotation(\n",
    "                            x, degree, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                X_aug.append(rotated)\n",
    "                y_aug.append(label)\n",
    "\n",
    "\n",
    "                if self.rot_and_shear:\n",
    "                    # random shears up to 40% intensity        \n",
    "                    for sh in range(*self.shear):\n",
    "                        sh /= 10\n",
    "                        sheared = tf.contrib.keras.preprocessing.image.random_shear(\n",
    "                                rotated, sh, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                        X_aug.append(sheared)\n",
    "                        y_aug.append(label)\n",
    "\n",
    "            # random shifts 20% left, right, up or down\n",
    "            for xsh in range(*self.x_shifts):\n",
    "                xsh /= 10\n",
    "                for ysh in range(*self.y_shifts):\n",
    "                    ysh /= 10\n",
    "                    shifted = tf.contrib.keras.preprocessing.image.random_shift(\n",
    "                            x, xsh, ysh, row_axis=0, col_axis=1, channel_axis=2)\n",
    "                    X_aug.append(shifted)\n",
    "                    y_aug.append(label)\n",
    "\n",
    "        X_aug = np.array(X_aug)\n",
    "        y_aug = np.array(y_aug)\n",
    "\n",
    "        # print(\"after TF, shapes = %s, %s\" % (X_aug.shape, y_aug.shape))\n",
    "        return (X_aug, y_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Class as SkLearn Estimator + Pipeline\n",
    "We made a class that wraps LeNet as a sklearn estimator, and performed a gridsearch in conjunction with the data augmentation transformer to find an optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataAugment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-615b53073d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m }\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataAugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataAugment' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from ext_test import kaggle_test\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "##LOAD INPUT\n",
    "X = np.load(\"x_mnist1000.npy\")\n",
    "X = X.reshape((-1, 28, 28, 1))\n",
    "y = np.load(\"y_mnist1000.npy\")\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "indices = np.random.permutation(len(X))\n",
    "\n",
    "valid_ind = indices[:100]\n",
    "test_ind = indices[100:200]\n",
    "train_ind = indices[200:]\n",
    "\n",
    "X_train = X[train_ind]\n",
    "y_train = y[train_ind]\n",
    "\n",
    "X_validation = X[valid_ind]\n",
    "y_validation = y[valid_ind]\n",
    "\n",
    "X_test = X[test_ind]\n",
    "y_test = y[test_ind]\n",
    "\n",
    "\n",
    "\n",
    "default_activations = [\n",
    "    \"conv1\",\n",
    "    \"avg_pool\",\n",
    "    \"conv2\",\n",
    "    \"avg_pool2\",\n",
    "    \"conv3\",\n",
    "    \"dense\",\n",
    "]\n",
    "\n",
    "elu = {x: tf.nn.elu for x in default_activations}\n",
    "selu = {x: tf.nn.selu for x in default_activations}\n",
    "relu = {x: tf.nn.relu for x in default_activations}\n",
    "tanh= {x: tf.nn.tanh for x in default_activations}\n",
    "\n",
    "param_grid = {\n",
    "    \"aug__rotation\": [[-60, 60, 10], [-30, 30, 10]],\n",
    "    \"aug__shear\": [[0, 50, 10], [0, 20, 10]],\n",
    "    \"aug__rot_and_shear\": [True, False],\n",
    "    \"clf__activations\": [selu, elu, tanh],\n",
    "    \"clf__dropout_rate\": [0.25, 0.35, 0.45],\n",
    "    \"clf__batch_size\": [500],\n",
    "    \"clf__using_da\": [True],\n",
    "}\n",
    "\n",
    "da = DataAugment()\n",
    "clf = CNNClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"aug\", da),\n",
    "        (\"clf\", clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "now = datetime.now().strftime(\"%H-%M-%S\")\n",
    "log_name = \"./logs/cnn_log_\" + now + \".log\"\n",
    "with open(log_name, \"w\") as log:\n",
    "    log.write(\"Param grid:\\n\")\n",
    "    for p in param_grid:\n",
    "        log.write(\"%s: %s\\n\" % (p, param_grid[p]))\n",
    " \n",
    "    gs = GridSearchCV(pipe, param_grid=param_grid)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_msg = \"best score: %s, params: %s\" % (gs.best_score_, gs.best_estimator_.get_params())\n",
    "    print(best_msg)\n",
    "    log.write(best_msg)\n",
    "\n",
    "    test_msg = \"Test accuracy: %s\" % gs.best_estimator_.score(X_test, y_test)\n",
    "    print(test_msg)\n",
    "    log.write(test_msg)\n",
    "    \n",
    "df = pd.DataFrame(gs.cv_results_)\n",
    "df.to_csv(\"./gs_results/\" + now + \".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters + Performance Plots\n",
    "\n",
    "We performed hyperparameter grid searches both with and without data augmentation. We used 5-fold cross validation and trained all models with a fixed number of samples so that those with augmented training data wouldn't get inflated scores from more training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Hyperparameters with no Data Augmentation\n",
    "Using the original training samples (no augmentation) and varying the classifier hyperparameters, these were the results of our grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"clf__activations\": [tf.nn.selu, tf.nn.elu, tf.nn.relu, tf.nn.tanh],\n",
    "    \"clf__learning_rate\": [0.01, 0.005, 0.05],\n",
    "    \"clf__dropout_rate\": [0, 0.1, 0.25, 0.45],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Best cv score:__ 96.67  \n",
    "__Test accuracy:__ 0.89  \n",
    "__Best parameters:__  \n",
    "activations: selu  \n",
    "learning rate: 0.01  \n",
    "dropout_rate: 0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Hyperparameters for Data Augmentation\n",
    "\n",
    "Using the classifier hyperparameters above and varying the augmentation hyperparameters, these were the results of our grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-611c6e1f10c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"aug__rot_and_shear\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"aug__n_iter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"clf__activations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"clf__learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"clf__dropout_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"aug__rotation\": [[30], [60], [90]],\n",
    "    \"aug__shear\": [[0], [20], [40]],\n",
    "    \"aug__rot_and_shear\": [True, False],\n",
    "    \"aug__n_iter\": [10],\n",
    "    \"clf__activations\": [tf.nn.selu],\n",
    "    \"clf__learning_rate\": [0.01],\n",
    "    \"clf__dropout_rate\": [0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Best cv accuracy:__  99.44   \n",
    "__Test accuracy:__ 0.93  \n",
    "__Best parameters:__    \n",
    "rotation: 90  \n",
    "shear: 40  \n",
    "rot_and_shear: False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Difference with data augmentation\n",
    "We also did a grid search to compare performance with and without data augmentation:\n",
    "![alt text](figures/rot_plot.jpg)\n",
    "\n",
    "Here the rotation hyperparameter is represented as a tuple (bottom, top, step) for the range of rotations to try. \n",
    "\n",
    "The bar plot above shows the mean (5-fold) validation scores for 32 models. \n",
    "\n",
    "![alt text](figures/200kaugval.jpg)\n",
    "The plot above shows the validation score for every batch up to 200,000 iterations for two models, one with data augmentation but identical otherwise. The addition of randomly rotated training samples clearly boosts the validation accuracy for this CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Rotation ranges\n",
    "These plots show the mean cross validation scores for models trained with data augmented by random rotations in a range or tuple of ranges.  \n",
    "__Training on 10k samples__:  \n",
    "![alt text](figures/lenet_augrot3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Training on 20k samples__:  \n",
    "![alt text](figures/lenet_augrot5fold.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Training on 40k samples__:  \n",
    "![alt text](figures/lenet_augrot_niter5_40k.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no clear ordering of these ranges, and the minor differences are probably due to the random nature of the keras preprocessing functions. The validation accuracy reported here is GridSearchCV.cv\\_results\\_.mean\\_test\\_score (test meaning against the test/validation fold).  \n",
    "\n",
    "cv\\_results\\_ also reports std\\_test\\_score, and for the last chart shown here, its mean across the six hyperparameter values is 0.012, while the range of mean scores is only 0.005."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
